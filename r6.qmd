---
title: "R6: Advanced data management"
author: "Modesto"
date: "August 16, 2023"
date-modified: "`r format(Sys.time(), '%Y-%m-%d (%H:%M h)')`"
categories: 
  - Data management
  - Data transformation
  - tabulation
  - aggregation
  - Large datasets
  - apply
format: 
  html:
    page-layout: full

toc: true
toc-location: left
toc-depth: 3
number-sections: true
number-depth: 2
code-overflow: wrap
link-external-icon: true
link-external-newwindow: true 
---

```{r wrap-hook, echo=FALSE}
library(knitr)
library(formatR)

hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "--- Cropped output ---"
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
  #  x <- c(more, x[lines], more)
      x <- c(x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```

# Transform dataframes

When you want to explore, present, analyze, or transform your data, you often need to subdivide, aggregate, or sort a vector or data frame by one or more variables. Currently, the R universe is essentially built on two (seemingly contradictory) approaches: Base R and `tidyverse` libraries (`dplyr()` & `tidyr()`) Although these two approaches are often seen as two different philosophies, they are meant to work together (see ref. 8 [below](#sec-references)).

We suggest some examples and tricks from Base R in this lesson because we believe that learning to work with Base R provides a broader range of capabilities for different types of data and work environments.

## Sorting and subsetting data

How come? Numeric? Alphabetic? increasing or decreasing order?

There are two main functions in R to order your data, `sort()` and `order()`. Let's see an example using the file [*coli_genomes_renamed.csv*](data/coli_genomes_renamed.csv), that we used in the lesson [R3](r3.html#save).

```{r}
#load the dataset
coli_genomes <- read.csv(file = "data/coli_genomes_renamed.csv", strip.white = TRUE)

#test sort() & order() in a vector
order(coli_genomes$Year)
sort(coli_genomes$Year)
sort(coli_genomes$Year, decreasing=TRUE)
order(coli_genomes$Phylogroup)
sort(coli_genomes$Phylogroup)
```

As you noticed, the R function `order()` returns a permutation of the order of the elements of a vector. The output is an index vector, not the vector itself. Also note that if the vector contains any NA values, they will be at the end of the index vector by default.

On the other hand, the function `sort()` returns returns the vector you pass as input sorted in ascending order by default.

Both functions order the data increasingly, and can be used for numeric or string variables. If you set the `decreasing` argument to `TRUE` in a `sort()` or `order()`, you will geet the vector of indices in descending order.

You can also use these functions to order a data frame according to a vector. Alternatively, you can also arrange or *shuffle* a data frame in reverse order.

```{r}
#order the dataframe by one column, sort or order?
coli_genomes[order(coli_genomes$Year),]
coli_genomes[sort(coli_genomes$Year),] #what happened?

# Random order of rows with sample 
coli_genomes[sample(nrow(coli_genomes), replace = FALSE), ]
# Reverse order of rows
coli_genomes[nrow(coli_genomes):1, ]
```

Now imagine you have a large data set and want to subdivide only some cases (=rows). You already know some ways to do this, but in the following example we will test the function 'subset()', which is very handy when you want to extract the cases that satisfy multiple conditions.

Imagine that for a new project we want to select the strains with better genome design quality or from a subset. We will keep strains from Phylogroup A, with an assembly N50 \> 150,000 bp and less than 100 contigs \>= 1 kb.

```{r}
#option 1: multi-step
selection<-coli_genomes[coli_genomes$contigs1kb<100,]
selection<-selection[selection$N50>150000,]
selection<-selection[selection$Phylogroup=='A',]

#option 2:  which statements 
selection2<-coli_genomes[which(coli_genomes$contigs1kb<100 & coli_genomes$N50>150000 & coli_genomes$Phylogroup=='A'),]

#option 2b:with attach(dataframe) you avoid repeat the name of the dataframe
attach(coli_genomes)
selection2 <- coli_genomes[which(contigs1kb<100 & N50>150000 & Phylogroup=='A'),]
detach(coli_genomes) #detach dataframe

#option 3: subset() is a straightforward way to do that trick 
selection3<-subset(coli_genomes, subset= contigs1kb<100 & N50>150000 & Phylogroup=='A')

```

If you want to make sure that the three methods give the same output, you can check the data frame, or check the dimensions or structure with `dim()` or `str()`, but this is difficult with large data sets with hundreds or thousands of cases (see below). The function `all.equal()` does this task for you.

```{r}
#Are they the same?
all.equal(selection,selection2) 
all.equal(selection,selection3)
```

## Combining dataframes

If you work with a lot of data or get different batches of related data frames, you may have multiple tables and want to combine them in different ways. An easy way is to use `cbind()` or `rbind()` if you have the same number of rows and columns. A more practical way is to use the `merge()` function. Look and test the following examples using the *coli_genomes* data frame and a new table named *colis3.csv*

```{r, error=TRUE}
colis <- read.csv2(file = 'data/colis3.csv')
#rbind() and cbind()
rbind(coli_genomes,colis) #did it work? why??
cbind(coli_genomes, colis) 

#merge()
merge<-merge(coli_genomes,colis) #inner join
str(merge)
merge1<-merge(coli_genomes,colis, by="Strain")
str(merge1)
merge2<-merge(coli_genomes,colis, by="Year")
str(merge2)
merge3<-merge(coli_genomes,colis, by=c("Strain","Year"))
str(merge3)
merge4<-merge(coli_genomes,colis, all=TRUE) #outer join
str(merge4)
merge5<-merge(coli_genomes,colis, all.x=TRUE)
str(merge5)
merge6<-merge(coli_genomes,colis, all.y=TRUE)
str(merge6)
merge7<-merge(coli_genomes,colis, by=NULL)
str(merge7)
```

As you noticed, the function `merge()` combines dataframes, however it is a generic function that can be also used with other objects (like vectors or matrices), but they will be coerced to `data.frame` class.

By default, `merge()` will do a *natural join* or *inner join*, merging two dataframes in one that contains only once the common elements of both. The arguments `all=TRUE`, `all.x=TRUE` and `all.y=TRUE` will force an *outer* join in which all elements of both, the first or the second dataframes are selected. In these cases, if not all rows in the first data frame match all the rows in the second, the output is filled with `NA` values in those cases.

Finally, the Cartesian product of both dataframes can be obtained in R setting as `NULL` the argument `by` of the `merge()` function.

### [Quick exercise (I)]{style="color:darkseagreen"}

Two students enrolled in different University courses: Rafa in A, B, D, E and G and Roger in B, C, D and F. Rafa got the following marks 8, 9, 9.5, 8.75 and 9 and Roger obtained 10, 9.7, 9 and 10. Write a R program to create merged dataframes with all the marks (outer join), all the Rafa's marks (left outer), all the Roger's marks (right outer), and the marks in the common courses (inner join).

```{r}
#Create the dataframes
rafa <- data.frame(c("A","B","D","E","G"),c(8,9,9.5,8.75,9.5))
roger <- data.frame(c("B","C","D","F"),c(10,9.7,9,10))
names(rafa) <- c("Course","Rafa")
names(roger) <- c("Course","Roger")
```

```{r wd, echo = FALSE, results = 'asis'}
opts_p <- c("merge(rafa, roger, by = \"Student\")", answer =  "merge(rafa, roger, by = \"Course\")",
   answer = "merge(rafa, roger)",
   "merge(rafa, roger, by = \"Course\", all=TRUE)")

cat("**How would you obtain a table containing the marks of both students only for their common courses?**",longmcq(opts_p))
```

```{r wd2, echo = FALSE, results = 'asis'}
opts_p <- c("merge(rafa, roger, by = \"Student\")",  "merge(rafa, roger, all.y=TRUE)",
   answer = "merge(rafa, roger, all.x=TRUE)",
   "merge(rafa, roger, by = \"Course\", all=TRUE)")

cat("**How would you obtain a table containing the marks of both students only for Rafa's courses?**",longmcq(opts_p))
```

## Data matrix format (wide vs. long table) {#zebra}

::: {.callout-important icon="false"}
## {{< bi question-circle >}} How should I arrange my data table?

A column per experimental condition or a column per variable? Is the same?
:::

In the table [*Zebrafish_data.csv*](data/Zebrafish_data.csv) from the Lesson R5 we have the results of an experiment in which a collaborator scored the number of metastatic cancer cells upon the expression of different transcripts of the EFNA3 gene. Each transcript is cloned into a pLoC plasmid, and we have negative (empty plasmid) and positive controls (wt transcript). Let's import and check the data.

```{r}
ZFdata <- read.csv("data/Zebrafish_data.csv")
str(ZFdata)
head(ZFdata)
```

How many columns has the table? How many variables are there?

Here we have one column per experimental condition. Thus, this table format does not agree with the standard way to introduce the data in a table, the data-matrix: Rows for cases and columns for variables.

Having the data, we transform it in a datamatrix (aka *long* table) using the function `stack()`. You can also do it the other way around with the function `unstack().`

```{r}
ZF_stacked <- stack(ZFdata)
str(ZF_stacked)
head(ZF_stacked)
ZF_old <- unstack(ZF_stacked)
str(ZF_old)
head(ZF_old)
```

However, some times the dataset is more complex and there are other variables that change the table structure, as in the table [*Zebrafish_full.csv*](data/Zebrafish_full.csv)*.* In this case, remember that stack is designed to transform tables of only two variables. Thus, you must stack only the desired two variables and then final construct the table by adding the rest of the information, for instance with `cbind()`, as in the example below.

```{r}
ZF_full <- read.csv("data/Zebrafish_full.csv", row.names=1)
head(ZF_full)
#option 1:
#step1: stack two main variables (sample & number of cells)
ZF_full_some_stack <- stack(ZF_full[,1:6])
head(ZF_full_some_stack,15)
#step2: reconstruct the table adding the third variable
ZF_full_stack <- cbind(ZF_full_some_stack,ZF_full[,7])
colnames(ZF_full_stack) <- c("Cells","Sample","Assay")
head(ZF_full_stack,15)
#option2: shorcut
ZF_full_stack2 <- cbind(stack(ZF_full[,1:6]),ZF_full[,7])
colnames(ZF_full_stack2) <- c("Cells","Sample","Assay")
head(ZF_full_stack2,15)
#the same result in both cases?
all.equal(ZF_full_stack,ZF_full_stack2)
```

Remember that ***datamatrix*** is not an R structure, but a general concept in data analysis. Also, for more complex dataframes, I suggest to use the function `melt()`. However, this function name refers to two different functions in two alternative packages: *reshape2* and *data.table*, with slightly different behavior.

### [Quick exercise (II)]{style="color:darkseagreen"}

From the previous exercise with marks of two students, we have a table `students` with all the marks, in which each student is a column.

```{r}
students <-  merge(rafa, roger, by = "Course", all = TRUE)
students
```

However, we need a table in which each column is a variable, as follows:

```{r echo=FALSE, eval=TRUE}
students2 <- cbind(students[,1], stack(students[,2:3]))
names(students2) <- c("Course","Mark","Student")
students2
```

How would you obtain the second table?

```{r, echo = TRUE, error = TRUE, eval = TRUE, webex.hide = "See an answer"}
#1. Create the table
students2 <- cbind(students[,1], stack(students[,2:3]))
#Note that we have used stack() for only two columns and bind the output with the column 1 in the previous table

#2. Rename variables
names(students2) <- c("Course","Mark","Student")

#3. Show table
students2
```

# Working efficiently with large datasets

## Fast loading of huge datasets and creation of contingency tables

We are going to make some examples with a huge dataset of Covid19 Vaccination data in EU. The data is publicly available at <https://www.ecdc.europa.eu/en/publications-data/data-covid-19-vaccination-eu-eea>. While you can download and open a file from internet in R, in order to use the same dataset, we have a file [*vaccines_EU_22oct2022.csv*](../data/vaccines_EU_22oct2022.csv) in our data folder.

::: callout-note
From this point on, in some examples we will show only some lines of the output of the code chunks (you will notice that the output ends with ***--- Cropped output ---***). Otherwise, this document is very long and difficult to follow.

As always, it's encouraged to run the code yourself to see and understand the full returned output.
:::

```{r error=TRUE, output.lines=(1:20)}
#open directly from the ECDC
#vaccines <- read.csv(file = "https://opendata.ecdc.europa.eu/covid19/vaccine_tracker/csv/data.csv", header=TRUE)
#we will use all the same dataset that I already downloaded
vaccines <- read.csv(file = 'data/vaccines_EU_22oct2022.csv', header=TRUE)
#explore the data
str(vaccines)
head(vaccines)

```

Even with large datasets, we can use `table()` or `summary()` functions, to overview data. But, sometimes we just doesn't.

```{r error=TRUE}
table(vaccines$Region)
```

```{r output.lines=(1:30)}
head(table(vaccines$Region, vaccines$NumberDosesReceived)) 
```

```{r error=TRUE}
summary(vaccines[which(vaccines$Region=="ES"),][,6]) #see the data from Spain
table(vaccines)
```

When exploring (very) large datasets as in the example above you can find that some functions don't work because they can't handle all that data (*Error in table(vaccines) : attempt to make a table with \>= 2\^31 elements*). Here, we are going to use alternative methods, more efficient and convenient for reading and handle these large datasets.

One of those tricks is the use of the package *data.table*, very handy for large datasets. In the following lines, we will compare reading/writting data with the function `fread()` and `fwrite().`

```{r}
#load the package may require installing it before
if(!require(data.table)){
    install.packages("data.table")
}
# if that doesn't work try: install.packages("data.table", type = "source", repos = "https://Rdatatable.gitlab.io/data.table")

system.time(vaccines <- read.csv(file = 'data/vaccines_EU_22oct2022.csv', header=TRUE))
system.time(vaccines<-fread('data/vaccines_EU_22oct2022.csv'))

#we can also prevent loading of some columns  to save time
system.time(vaccines2 <-fread('data/vaccines_EU_22oct2022.csv',drop=c(3,5,7,9,10)))  
system.time(vaccines3 <-fread('data/vaccines_EU_22oct2022.csv',colClasses = "character")) #we can also select the variables by type
```

## Frequency tables: `table()` vs. `xtabs()`

Tabulate frequencies is very common and quick way to obtain information from a dataset. A good alternative for complex tabulation and/or large datasets is the funcion `xtabs()`. The output of both `xtabs()` and `table()` is nearly the same, although `xtabs()` is more convenient for complex cross tabulations. Still some users prefer the use of derivative `table()` uses, like including it within the `with()` function. I believe that `xtabs()` provides a more straightforward alternative. Moreover, `xtabs()` has interesting advantages: 1) row and column labels are included automatically, set to the variable names and 2) there is a data= argument, which means you just have to reference the variable names. With `xtabs()`, you do not list out the variables of interest separated by commas. Instead you use formula notation, which is \~variable1+variable2+... where variable1 and variable2 are the names of the variables of interest.

```{r }
#compare table() and xtabs()
table(vaccines$ReportingCountry)
str(table(vaccines$ReportingCountry))
xtabs(formula= ~ ReportingCountry  ,data=vaccines)
str(xtabs(formula= ~ ReportingCountry  ,data=vaccines))
#Note: library(stats) if xtabs() does not work
```

So far the seem very similar in terms of coding and obtained output. However, what about if you want to use to variables?

```{r output.lines=(1:25)}
#with 2 or more variables?
table(vaccines[,c(ReportingCountry,as.factor(Vaccine))]) #not really

## you can use "with", but not so intuitive to me
with(vaccines, table(ReportingCountry, as.factor(Vaccine))) 


#more strightforward alternative with xtabs
(vaccines_split <- xtabs(formula= ~ ReportingCountry  + as.factor(Vaccine), data = vaccines))

```

As shown above, `xtabs()` can be used with more than two variables, although the resulting cross-tabulated table may be too long. Moreover, you can create a cross-tabulated table using a numeric variable instead of frequencies. To do so, you just need to use the left side of the formula.

```{r output.lines=(1:50)}
#also with several variables
(vaccines_split2 <- xtabs(NumberDosesReceived ~ ReportingCountry  + as.factor(Vaccine) + as.factor(TargetGroup), vaccines))
```

Then, if you want to show your tabulated data in a more useful format, you can covert the cross-tabulated data into a flat table with `ftable()`. See the examples below and try to understand how the `row.bars` statement works.

```{r output.lines=(1:25)}
ftable(vaccines_split, row.vars = 2)
ftable(vaccines_split2, row.vars = 2)
ftable(vaccines_split2, row.vars = 3)
```

### [Quick exercise (III)]{style="color:darkseagreen"}

Sometimes, it is better to use a shorter table as an example... Let's go back to the *coli_genomes* data that you already know.

```{r wd_genomes, echo = FALSE, results = 'asis'}
opts_p <- c("with(coli_genomes, table(Source, Phylogroup))",
   answer = "xtabs(Contigs ~ Source + Phylogroup, coli_genomes)",
   "xtabs( ~ Contigs + Source + Phylogroup, coli_genomes)")

cat("**Could you obtain a table like the following, with the number of Contigs by isolation source (*Source*) and phylogenetic group (*Phylogroup*)?**",longmcq(opts_p))
```

```{r echo=F, eval=T}
xtabs(Contigs ~ Source + Phylogroup, coli_genomes)
```

Now, let's do it a little bit more complicated. If we split the previous table by the STs (aka. multilocus sequence typing), you obtain a very large table:

```{r echo=F, eval=T, output.lines=(1:50)}
(coli_xgenomes <- xtabs(Contigs ~ Source + Phylogroup + Sequence.Type, coli_genomes))
```

This is quite a long cross-table that we have cropped on the output, maybe you could find more convenient transforming it to a flat contingency table with `ftable()`.

```{r wd_xgenomes, echo = FALSE,  results = 'asis'}
opts_p <- c("ftable(coli_xgenomes)",
   "ftable(coli_xgenomes, col.vars=1",
  answer = "ftable(coli_xgenomes, row.vars=3",
  answer = "ftable(coli_xgenomes, col.vars=c(1,2)"
  )

cat("**How would you generate a flat table using the previous cross-tabulated table?**",longmcq(opts_p))
```

If you test the different options, you will notice that the format of the table can be defined by the order of the variables and also using the arguments `row.vars` and `col.vars` to define which variables will be summarized as column or row.

Again, rather than concepts from R or RStudio, cross-tabulation and flat contingency tables are general concepts in data analysis.

# Data aggregation and transformation

## By() & aggregate()

While frequency tables can be very quickly generated with `table()` and `xtable()`, sometimes you may want other calculations than frequency, like basic statistics per group or other, even custom, calculations.

You have already tried the function `by()` as a very useful trick to make group-calculations. However, this function has some limitations when we have large datasets and when we try to use multiple factors that can be solved with `aggregate()`. In both cases, the function can be any function that suits your data, either from R packages or a custom function.

In the following example, we are using a smaller dataset named *vacines2* that we can obtain by *sampling* (randomly extraction of a portion of data) with the R function `sample()`.

```{r error=TRUE, output.lines=(1:25)}
vaccines2 <- vaccines[sample(x = 1:nrow(vaccines),size = 50000, replace=FALSE),]

#using by()
by(vaccines2$NumberDosesReceived, INDICES = vaccines2$Region, FUN=mean, na.rm=TRUE)
```

In the example above, we could calculate the mean by groups. Can we do it with more than one grouping factor? Let's try some alternatives and see the output

```{r error=TRUE, output.lines=(1:25)}
by(vaccines2$NumberDosesReceived, INDICES = vaccines2$Region + vaccines2$Vaccine, FUN=mean, na.rm=TRUE)
by(vaccines2$NumberDosesReceived, INDICES = c(vaccines2$Region,vaccines2$Vaccine), FUN=mean, na.rm=TRUE)
by(vaccines2$NumberDosesReceived, INDICES = list(vaccines2$Region,vaccines2$Vaccine), FUN=mean, na.rm=TRUE)
```

When you find difficult using `by()`, `aggregate()` is usually the best quick alternative

```{r,  output.lines=(1:15)}
#aggregate is more convenient sometimes
aggregate(vaccines2$NumberDosesReceived ~ vaccines2$Region, FUN=mean)
aggregate(NumberDosesReceived ~ Region+Vaccine, data=vaccines2, FUN=mean) 
aggregate(FirstDoseRefused ~ TargetGroup, data=vaccines2, FUN=median) 
#note that by default NAs are disregarded
aggregate(FirstDoseRefused ~ TargetGroup, data=vaccines2, na.action = NULL, FUN=median) 

#we can make it for several numeric variables at the same time
aggregate(cbind(NumberDosesReceived,FirstDose,SecondDose) ~ as.factor(TargetGroup), data=vaccines2, FUN=mean)
```

We can also add a custom function, using the short, usually with the inline notation:

```{r }
#we can also use custom functions here
aggregate(cbind(FirstDose,SecondDose) ~ as.factor(TargetGroup), data=vaccines2, FUN=function(x) mean(x)*100/mean(vaccines2$NumberDosesReceived, na.rm=TRUE))
```

### [Quick exercise (IV)]{style="color:darkseagreen"}

We are using again the *coli_genomes* dataframe.

```{r wd_genomes_agg, echo = FALSE, results = 'asis'}
opts_p <- c("aggregate(cbind(VF,Plasmids) ~ Source, data=coli_genomes, FUN=mean)",
            "aggregate(cbind(VF,Plasmids) ~ Source, data=coli_genomes, FUN=function(x) mean(x) * 100 / mean(Assembly_length))",
   answer = "aggregate(cbind(VF,Plasmids) ~ Source, data=coli_genomes, FUN=function(x) mean(x)  * 100 / mean(coli_genomes$Assembly_length))" )

cat("**Can you obtain the  following table displaying the mean (%) of virulece factors (VF) and plasmids per genome nucleotide (Assembly_length) grouped by Source**",longmcq(opts_p))
```

```{r echo=F, eval=T}
aggregate(cbind(VF,Plasmids) ~ Source, data=coli_genomes, FUN=function(x) mean(x)*100/mean(coli_genomes$Assembly_length))
```

## Apply family of functions

::: {style="float: left; position: relative; top: 0px; padding: 3px;"}
[![Apply family of functions](images/apply_family-01.jpg){width="428"}](https://www.r-bloggers.com/2016/03/apply-lapply-rapply-sapply-functions-in-r-2/)
:::

The functions of the **Apply** family are designed to apply a function to each element of a data structure. It includes various functions, designed for matrices, dataframes or lists. What is special about these functions is that their arguments are not only data but also functions. Using an **apply** function can sometimes seem complicated, but once you get used to it, it's actually very handy and faster than other alternatives, such as loops. In this section, we look at the `apply()` function and its variants, including `lapply()`, `sapply()`, `tapply()`, `rapply()` & `mapply()`. In general, **Apply** is a very efficient tool to perform repetitive calculations.

Rather than describing in detail all the functions, in the following examples we test some of the possibilities of some *applies*. Try yourself and check the code comments.

```{r, output.lines=(1:6)}
#apply()
apply(vaccines[,4:12], MARGIN = 1, FUN = mean, na.rm=TRUE)
apply(vaccines[,4:12], 2, mean, na.rm=TRUE) # 1 for row-wise operation, 2 for column-wise
```

```{r}
apply(vaccines, 2, is.numeric)
apply(vaccines[,3:6], 2, is.numeric)
```

Note that you can use any function, although `apply()`is best suited for numeric calculations as it coerces your data to an array (via `as.matrix()`). Thus, when a dataframe cannot be transformed in a matrix your function may return fail. In the example above `is.numeric()` returns false for all columns within the apply when the dataframe contains character data, but if we use a subset that only contains numeric data the result makes sense. As you can see below, `sapply()` and `lapply()` are best suited for this kind of functions.

```{r error=TRUE,output.lines=(1:25)}
#lapply() and sapply() are very useful
lapply(by(vaccines$Population,vaccines$ReportingCountry,mean), log10) 
```

```{r error=TRUE}

sapply(by(vaccines$Population,vaccines$ReportingCountry,mean), log10) 
class(lapply(by(vaccines$Population,vaccines$ReportingCountry,mean), log10) ) 
#lapply take a list (or object that can be coerced as one) and gives a list
class(sapply(by(vaccines$Population,vaccines$ReportingCountry,mean), log10) ) 
#sapply is actually a 'wrapper' of lapply that returns a vector or matrix instead of a list


sapply(list(vaccines2$Population, vaccines2$FirstDose,vaccines2$SecondDose, vaccines2$NumberDosesReceived), median) # we can do it with several variables
sapply(vaccines2,class) #again, you can use any function, not only numeric calculations
sapply(vaccines2, is.numeric) 
```

```{r error=TRUE,output.lines=(1:10)}
#other apply functions are more case-specific:

#mapply() is the matrix (or multivariable) version
mapply(sub,vaccines2[,4],vaccines2[,6],vaccines2[,7]) 
```

```{r error=TRUE}

mapply(rep, letters[1:4], 4:1)

#tapply() breaks a vector into pieces, can be used instead of aggregate
tapply(vaccines$NumberDosesReceived, vaccines$ReportingCountry, median,na.rm=TRUE) 
tapply(coli_genomes$VF, coli_genomes$Source, mean, na.rm=TRUE) 
```

```{r error=TRUE}
#vapply() is similar to sapply(), but has a pre-specified type of return value
vapply(vaccines[,8:11],mean,numeric(1))
```

```{r error=TRUE,output.lines=(1:10)}

vapply(vaccines[,8:11],function(x) x^2,numeric(nrow(vaccines)))

```

```{r error=TRUE}

#rapply can be a handy trick for recursive calculations
rapply(vaccines, mean, class="integer") 
rapply(vaccines, table, class="factor")
```

```{r error=TRUE,output.lines=(1:10)}


vaccines$ReportingCountry<-as.factor(vaccines$ReportingCountry)
vaccines$Vaccine<-as.factor(vaccines$Vaccine)
vaccines$TargetGroup<-as.factor(vaccines$TargetGroup)
vaccines$Region<-as.factor(vaccines$Region)
rapply(vaccines, table, class="factor") #factors need to be defined and now it gives the counts for each group of each factor

rapply(vaccines, median, how="list", class="integer", na.rm=TRUE) #returns a list

```

```{r error=TRUE}

rapply(vaccines, median,  how="unlist",class="integer", na.rm=TRUE) #gives a vector


#what do you mean by RECURSIVELY?
x <- list("a",list(24,443),434,list(54,list(6443,7234))) #this is a complex list with sublists

str(x)

rapply(x,log2,class=c("numeric"))
(r <- rapply(x,log2,class=c("numeric"), how="unlist"))
str(r)
```

Note that `tapply()` and `by()` are very similar. Indeed, `by()` is a wrapper of `tapply()`. On the other hand, `vapply()` is similar to `sapply`, but has a pre-specified type of return value, so it can be safer (or faster) to use sometimes. Finally, `rapply()` is not very often used, but it can be the best option for complex datasets, like those including nested lists or complex lists with several dataframes and/or vectors.

The applications of *apply* functions are very wide, particularly the `apply()` and `lapply()`. You can use them to parse data with many different input and output structures and almost any kind of R functions, including plots (see Lesson R8).

### [Quick exercise (V)]{style="color:darkseagreen"}

Of course, based on the *coli_genomes* dataframe.

```{r wd_apply, echo = FALSE, results = 'asis'}
opts_p <- c("apply(coli_genomes,2,log10(mean), na.rm=TRUE)",
            "apply(coli_genomes,1,log10(mean), na.rm=TRUE)",
            "lapply(coli_genomes,log10(mean), na.rm=TRUE)",
            "sapply(coli_genomes,log10(mean), na.rm=TRUE)",
   answer = "None of the previous" )

cat("**How would you obtain log10 of the mean of all the columns?**",longmcq(opts_p))
```

Yes, a function must be suitable for the selected data and the dataframe *coli_genomes* contains numeric and categorical variables, but the function *log10(mean)* only can work with numeric data. Let's do it in two steps:

```{r wd_apply2, echo = FALSE, results = 'asis'}
opts_p <- c("apply(coli_genomes, 2, is.numeric) ",
            "coli_genomes[lapply(coli_genomes, is.numeric)]",
   answer = "coli_genomes[sapply(coli_genomes, is.numeric)]",
    "sapply(coli_genomes, is.numeric)",
            "lapply(coli_genomes, is.numeric)")

cat("**How would you select only numeric columns?**",longmcq(opts_p))
```

```{r wd_apply3, echo = FALSE, results = 'asis'}
opts_p <- c("apply(coli_genomes[sapply(coli_genomes, is.numeric)],2,function(x)log(mean(x))) ",
            "apply(coli_genomes[sapply(coli_genomes, is.numeric)],2,log(mean)) ",
            "apply(coli_genomes[sapply(coli_genomes, is.numeric)],2,fun(x) log(mean), na.rm=TRUE)",
            answer= "apply(coli_genomes[sapply(coli_genomes, is.numeric)],2,function(x)log(mean(x, na.rm=TRUE)))" )

cat("**How would you obtain log10 of the mean of all the numeric columns?**",longmcq(opts_p))
```

# References {#sec-references}

1.  Manipulación de datos en R: <https://r-coder.com/manipulacion-datos-r/> \[ES\] & <https://r-coder.com/r-data-manipulation/> \[EN\]

2.  *R in action.* Robert I. Kabacoff. March 2022 ISBN 9781617296055

3.  Working with tables in R: <https://bookdown.org/kdonovan125/ibis_data_analysis_r4/working-with-tables-in-r.html>

4.  About flat tables in R: <https://cran.r-project.org/web/packages/memisc/vignettes/ftable-matrix.html>

5.  *data.table* cheatsheet: <https://raw.githubusercontent.com/rstudio/cheatsheets/main/datatable.pdf>

6.  Data aggregation: <https://r-coder.com/aggregate-r/>

7.  About *Applies*: <https://www.r-bloggers.com/2016/03/apply-lapply-rapply-sapply-functions-in-r-2/> & <https://gist.github.com/lyndametref/4d137fcba1ec4d9af80ad53245b358ff>

8.  Efficient data management in R: <https://www.mzes.uni-mannheim.de/socialsciencedatalab/article/efficient-data-r/>

# [Exercises]{style="color:green"}

We are using the Covid19 test data for exercises. You can download the updated *csv* file from <https://opendata.ecdc.europa.eu/covid19/testing/csv/data.csv> (see also <https://www.ecdc.europa.eu/en/publications-data/covid-19-testing>). Read and explore the data. I also suggest to save the table in your computer as *covid_tests.csv*.

#### 1. Generate two contingency tables showing the number of tests done (1) by country per week and (2) by country per month.

Hint. For the second table, use the function *ISOweek2date()* from the package "ISOweek" to transform the ISO date_week to standard date format.

#### 2. Remove all the character columns to create a subset dataframe named *minitests*. You can use `subset()` and `sapply()` here.

#### 3. Use apply() to obtain the min, max, median and mean of each column.

#### 5. Use `tapply()` to obtain the mean, median and min of new_cases per country and construct a dataframe with the result.

Hint. Consider how to handle NA's.

# [Extra exercises]{style="color:green"}

The topics in this lesson can be dense and bewildering the first time. I selected some sheets of exercises from great websites that can give you an extra practice.

-   <https://www.r-exercises.com/2016/04/14/merging-dataframes-exercises/>

-   <https://www.r-bloggers.com/2016/05/cross-tabulation-with-xtabs-exercises/>

-   <https://www.r-exercises.com/2016/09/08/efficient-processing-with-apply-exercises/>

-   <https://www.r-exercises.com/2016/06/16/summary-statistics-with-aggregate/>

# Session Info

```{r}
sessionInfo()
```
